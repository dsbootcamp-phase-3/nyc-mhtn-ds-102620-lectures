{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization \n",
    "\n",
    "_December 8, 2020_\n",
    "\n",
    "Agenda today:\n",
    "- Ridge regression \n",
    "- Lasso regression \n",
    "- AIC and BIC"
   ]
  },
  {
   "source": [
    "y = Bo + B1X1 + e<br />\n",
    "B1 represents the effect that X's effect on Y<br />\n",
    "residuals: y - y_hat<br />\n",
    "minimize the residuals, which is the cost function<br />\n",
    "residuals also known as MSE<br/>\n",
    "\n",
    "putting a penalty to cost function for linear regression, introduce a little bit of bias, but reduce the variance<br />\n",
    "essence of regularization<br />"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Ridge Regression\n",
    "\n",
    "hyperparameter lambda is decided by you, and beta is decided by the data<br />\n",
    "if lambda is zero, that means that beta is big<br />\n",
    "if lambda is one, then means beta has to decrease, but never goes to zero as lambda goes to infinity<br />\n",
    "determined by trial and error, tried a bunch of different values in order to produce the best RMSE<br />\n",
    "Lasso can go to zero, but Ridge cannot, beta will never go to zero<br />"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Regularizing a Model\n",
    "Even though Lasso and Ridge regressions are only used in regression, regularizing a model is a common procedure in the process of building machine learning models. It is an effectuve procedure for tackling the problem of overfitting. Generally speaking, applying regularization technique introduces some **bias** to the model, but reduces the **variance**, and therefore results in better performance in testing data. As you will see later in this module, models built from various classification algorithms often require tuning using regularization in order to overcome overfitting. \n",
    "\n",
    "What is regularization in the context of regression? As we recall, as the complexity of model increases, the model overfits and performance on the testing set decreases. Regularization techniques *shrinks* the regression coefficients such that the coefficients are not affecting the outcomes as much as they originally would have. In other words, using regularization applies a *penalty* to the coefficients of your regression model. Let's see how exactly Ridge regression and Lasso regression work to reduce variances in regression models and result in better fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Ridge Regression (L2 Norm)\n",
    "Before we dive into regularization, let's (re)visit a concept called **Cost Function**. A cost function is a measure of how good or bad the model is at estimating the relationship of our $X$ and $y$ variables. Usually, it is expressed in the difference between actual values and predicted values. For simple linear regression, the cost function is represented as:\n",
    "<center> $$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\sum( bx + b_0))^2$$\n"
   ]
  },
  {
   "source": [
    "minimize MSE plus something else, which is lambda a constant multiplied by beta^squared<br />\n",
    "lambda is equivalent to alpha in Python<br />\n",
    "constant usually from 0 to 1<br />\n",
    "introduce bias which is the residuals, we end up reducing the variance because the beta value is smaller<br />\n",
    "\n",
    "its a model that you would fit to the training set and then evaluate on both the train and test<br />\n",
    "we use gradient descent to determine beta<br />\n",
    "L2 is because of the squared<br />"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression with multiple predictors, the cost function is expressed as:\n",
    "$$ \\text{cost_function} = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2$$\n",
    "\n",
    "Where k stands for number of predictors at jth term.\n",
    "\n",
    "The ridge regression applies a penalizing parameter $\\lambda$ *slope* $^2$, such that a small bias will be introduced to the entire model depending on the value of $\\lambda$, which is called a *hyperparameter*. \n",
    "\n",
    "$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2 + \\lambda \\sum_{j=1}^p m_j^2$$\n",
    "\n",
    "The result of applying such a penalizing parameter to the cost function, resulting a different regression model that minimizing the residual sum of square **and** the term $\\lambda \\sum_{j=1}^p m_j^2$. \n",
    "\n",
    "The Ridge regression improves the fit of the original regression line by introducing some bias/changing the slope and intercept of the original line. Recall the way we interpret a regression model Y = mx + b: with every unit increase in x, the outcome y increase by m unit. Therefore, the bigger the coefficient m is, the more the outcome is subjected to changes in predictor x. Ridge regression works by reducing the magnitude of the coefficient m and therefore reducing the effect the predictors have on the outcome. Let's look at a simple example.\n",
    "\n",
    "The ridge regression penalty term contains all of the coefficients squared from the original regression line except for the intercept term. "
   ]
  },
  {
   "source": [
    "## Lasso Regression\n",
    "\n",
    "lambda times absolute value of beta instead of beta squared in order to introduce mse<br />\n",
    "we want lasso to be as low as possible<br />\n",
    "when beta becomes zero, it performs feature selection<br />\n",
    "goal is to minimize the sum of all betas ultimately<br />"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Lasso Regression (L1 Norm)\n",
    "Lasso regression is very similar to Ridge regression except for one difference - the penalty term is not squared but the absolute values of the coefficients muliplied by lambda, expressed by:\n",
    "\n",
    "$$ \\text{cost_function_lasso}=  \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\n",
    "\n",
    "The biggest difference in Ridge and Lasso is that Lasso simultaneously performs variable selection: some coefficients are shrunk to 0, rendering them nonexistence in the original regression model. Therefore, Lasso regression performs very well when you have higher dimensional dataset where some predictors are useless; whereas Ridge works best when all the predictors are needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/AWeYSE0qgpk76/giphy.gif\" width= \"400\" />\n",
    "only Lasso will beat the parameters for zero"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# implementation \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = sns.load_dataset('mpg')\n",
    "\n",
    "#data = pd.read_csv(\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-2-24-09-ridge-and-lasso-regression/master/auto-mpg.csv\") \n",
    "data = data.sample(50)\n",
    "data.dropna(inplace=True)\n",
    "y = data[[\"mpg\"]]\n",
    "X = data.drop([\"mpg\", \"name\", \"origin\"], axis=1)\n",
    "\n",
    "data = data.sample(50)\n",
    "scale = MinMaxScaler()\n",
    "transformed = scale.fit_transform(X)\n",
    "X = pd.DataFrame(transformed, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "109  21.0          4         140.0        72.0    2401          19.5   \n",
       "8    14.0          8         455.0       225.0    4425          10.0   \n",
       "38   14.0          8         350.0       165.0    4209          12.0   \n",
       "329  44.6          4          91.0        67.0    1850          13.8   \n",
       "189  15.5          8         304.0       120.0    3962          13.9   \n",
       "223  15.5          8         318.0       145.0    4140          13.7   \n",
       "233  29.0          4          97.0        78.0    1940          14.5   \n",
       "43   13.0          8         400.0       170.0    4746          12.0   \n",
       "310  38.1          4          89.0        60.0    1968          18.8   \n",
       "160  17.0          6         231.0       110.0    3907          21.0   \n",
       "318  29.8          4         134.0        90.0    2711          15.5   \n",
       "196  24.5          4          98.0        60.0    2164          22.1   \n",
       "91   13.0          8         400.0       150.0    4464          12.0   \n",
       "203  29.5          4          97.0        71.0    1825          12.2   \n",
       "164  21.0          6         231.0       110.0    3039          15.0   \n",
       "239  30.0          4          97.0        67.0    1985          16.4   \n",
       "77   22.0          4         121.0        76.0    2511          18.0   \n",
       "174  18.0          6         171.0        97.0    2984          14.5   \n",
       "105  13.0          8         360.0       170.0    4654          13.0   \n",
       "87   13.0          8         350.0       145.0    3988          13.0   \n",
       "215  13.0          8         318.0       150.0    3755          14.0   \n",
       "309  41.5          4          98.0        76.0    2144          14.7   \n",
       "175  29.0          4          90.0        70.0    1937          14.0   \n",
       "106  12.0          8         350.0       180.0    4499          12.5   \n",
       "148  26.0          4         116.0        75.0    2246          14.0   \n",
       "76   18.0          4         121.0       112.0    2933          14.5   \n",
       "44   13.0          8         400.0       175.0    5140          12.0   \n",
       "224  15.0          8         302.0       130.0    4295          14.9   \n",
       "65   14.0          8         351.0       153.0    4129          13.0   \n",
       "78   21.0          4         120.0        87.0    2979          19.5   \n",
       "88   14.0          8         302.0       137.0    4042          14.5   \n",
       "356  32.4          4         108.0        75.0    2350          16.8   \n",
       "288  18.2          8         318.0       135.0    3830          15.2   \n",
       "201  18.5          6         250.0       110.0    3645          16.2   \n",
       "392  27.0          4         151.0        90.0    2950          17.3   \n",
       "208  13.0          8         318.0       150.0    3940          13.2   \n",
       "308  33.5          4         151.0        90.0    2556          13.2   \n",
       "178  23.0          4         120.0        88.0    2957          17.0   \n",
       "11   14.0          8         340.0       160.0    3609           8.0   \n",
       "37   18.0          6         232.0       100.0    3288          15.5   \n",
       "232  16.0          8         351.0       149.0    4335          14.5   \n",
       "81   28.0          4          97.0        92.0    2288          17.0   \n",
       "26   10.0          8         307.0       200.0    4376          15.0   \n",
       "383  38.0          4          91.0        67.0    1965          15.0   \n",
       "321  32.2          4         108.0        75.0    2265          15.2   \n",
       "169  20.0          6         232.0       100.0    2914          16.0   \n",
       "112  19.0          4         122.0        85.0    2310          18.5   \n",
       "138  14.0          8         318.0       150.0    4457          13.5   \n",
       "54   35.0          4          72.0        69.0    1613          18.0   \n",
       "47   19.0          6         250.0       100.0    3282          15.0   \n",
       "\n",
       "     model_year  origin                        name  \n",
       "109          73     usa              chevrolet vega  \n",
       "8            70     usa            pontiac catalina  \n",
       "38           71     usa            chevrolet impala  \n",
       "329          80   japan         honda civic 1500 gl  \n",
       "189          76     usa                 amc matador  \n",
       "223          77     usa       dodge monaco brougham  \n",
       "233          77  europe    volkswagen rabbit custom  \n",
       "43           71     usa    ford country squire (sw)  \n",
       "310          80   japan       toyota corolla tercel  \n",
       "160          75     usa               buick century  \n",
       "318          80   japan      toyota corona liftback  \n",
       "196          76     usa             chevrolet woody  \n",
       "91           73     usa   chevrolet caprice classic  \n",
       "203          76  europe           volkswagen rabbit  \n",
       "164          75     usa               buick skyhawk  \n",
       "239          77   japan                   subaru dl  \n",
       "77           72  europe         volkswagen 411 (sw)  \n",
       "174          75     usa                  ford pinto  \n",
       "105          73     usa      plymouth custom suburb  \n",
       "87           73     usa            chevrolet malibu  \n",
       "215          76     usa                  dodge d100  \n",
       "309          80  europe                   vw rabbit  \n",
       "175          75  europe           volkswagen rabbit  \n",
       "106          73     usa    oldsmobile vista cruiser  \n",
       "148          74  europe                 fiat 124 tc  \n",
       "76           72  europe             volvo 145e (sw)  \n",
       "44           71     usa         pontiac safari (sw)  \n",
       "224          77     usa     mercury cougar brougham  \n",
       "65           72     usa            ford galaxie 500  \n",
       "78           72  europe            peugeot 504 (sw)  \n",
       "88           73     usa            ford gran torino  \n",
       "356          81   japan              toyota corolla  \n",
       "288          79     usa             dodge st. regis  \n",
       "201          76     usa          pontiac ventura sj  \n",
       "392          82     usa            chevrolet camaro  \n",
       "208          76     usa  plymouth volare premier v8  \n",
       "308          79     usa             pontiac phoenix  \n",
       "178          75  europe                 peugeot 504  \n",
       "11           70     usa          plymouth 'cuda 340  \n",
       "37           71     usa                 amc matador  \n",
       "232          77     usa            ford thunderbird  \n",
       "81           72   japan             datsun 510 (sw)  \n",
       "26           70     usa                   chevy c20  \n",
       "383          82   japan                 honda civic  \n",
       "321          80   japan              toyota corolla  \n",
       "169          75     usa                 amc gremlin  \n",
       "112          73     usa                  ford pinto  \n",
       "138          74     usa   dodge coronet custom (sw)  \n",
       "54           71   japan                 datsun 1200  \n",
       "47           71     usa            pontiac firebird  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mpg</th>\n      <th>cylinders</th>\n      <th>displacement</th>\n      <th>horsepower</th>\n      <th>weight</th>\n      <th>acceleration</th>\n      <th>model_year</th>\n      <th>origin</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>109</th>\n      <td>21.0</td>\n      <td>4</td>\n      <td>140.0</td>\n      <td>72.0</td>\n      <td>2401</td>\n      <td>19.5</td>\n      <td>73</td>\n      <td>usa</td>\n      <td>chevrolet vega</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14.0</td>\n      <td>8</td>\n      <td>455.0</td>\n      <td>225.0</td>\n      <td>4425</td>\n      <td>10.0</td>\n      <td>70</td>\n      <td>usa</td>\n      <td>pontiac catalina</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>14.0</td>\n      <td>8</td>\n      <td>350.0</td>\n      <td>165.0</td>\n      <td>4209</td>\n      <td>12.0</td>\n      <td>71</td>\n      <td>usa</td>\n      <td>chevrolet impala</td>\n    </tr>\n    <tr>\n      <th>329</th>\n      <td>44.6</td>\n      <td>4</td>\n      <td>91.0</td>\n      <td>67.0</td>\n      <td>1850</td>\n      <td>13.8</td>\n      <td>80</td>\n      <td>japan</td>\n      <td>honda civic 1500 gl</td>\n    </tr>\n    <tr>\n      <th>189</th>\n      <td>15.5</td>\n      <td>8</td>\n      <td>304.0</td>\n      <td>120.0</td>\n      <td>3962</td>\n      <td>13.9</td>\n      <td>76</td>\n      <td>usa</td>\n      <td>amc matador</td>\n    </tr>\n    <tr>\n      <th>223</th>\n      <td>15.5</td>\n      <td>8</td>\n      <td>318.0</td>\n      <td>145.0</td>\n      <td>4140</td>\n      <td>13.7</td>\n      <td>77</td>\n      <td>usa</td>\n      <td>dodge monaco brougham</td>\n    </tr>\n    <tr>\n      <th>233</th>\n      <td>29.0</td>\n      <td>4</td>\n      <td>97.0</td>\n      <td>78.0</td>\n      <td>1940</td>\n      <td>14.5</td>\n      <td>77</td>\n      <td>europe</td>\n      <td>volkswagen rabbit custom</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>13.0</td>\n      <td>8</td>\n      <td>400.0</td>\n      <td>170.0</td>\n      <td>4746</td>\n      <td>12.0</td>\n      <td>71</td>\n      <td>usa</td>\n      <td>ford country squire (sw)</td>\n    </tr>\n    <tr>\n      <th>310</th>\n      <td>38.1</td>\n      <td>4</td>\n      <td>89.0</td>\n      <td>60.0</td>\n      <td>1968</td>\n      <td>18.8</td>\n      <td>80</td>\n      <td>japan</td>\n      <td>toyota corolla tercel</td>\n    </tr>\n    <tr>\n      <th>160</th>\n      <td>17.0</td>\n      <td>6</td>\n      <td>231.0</td>\n      <td>110.0</td>\n      <td>3907</td>\n      <td>21.0</td>\n      <td>75</td>\n      <td>usa</td>\n      <td>buick century</td>\n    </tr>\n    <tr>\n      <th>318</th>\n      <td>29.8</td>\n      <td>4</td>\n      <td>134.0</td>\n      <td>90.0</td>\n      <td>2711</td>\n      <td>15.5</td>\n      <td>80</td>\n      <td>japan</td>\n      <td>toyota corona liftback</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>24.5</td>\n      <td>4</td>\n      <td>98.0</td>\n      <td>60.0</td>\n      <td>2164</td>\n      <td>22.1</td>\n      <td>76</td>\n      <td>usa</td>\n      <td>chevrolet woody</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>13.0</td>\n      <td>8</td>\n      <td>400.0</td>\n      <td>150.0</td>\n      <td>4464</td>\n      <td>12.0</td>\n      <td>73</td>\n      <td>usa</td>\n      <td>chevrolet caprice classic</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>29.5</td>\n      <td>4</td>\n      <td>97.0</td>\n      <td>71.0</td>\n      <td>1825</td>\n      <td>12.2</td>\n      <td>76</td>\n      <td>europe</td>\n      <td>volkswagen rabbit</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>21.0</td>\n      <td>6</td>\n      <td>231.0</td>\n      <td>110.0</td>\n      <td>3039</td>\n      <td>15.0</td>\n      <td>75</td>\n      <td>usa</td>\n      <td>buick skyhawk</td>\n    </tr>\n    <tr>\n      <th>239</th>\n      <td>30.0</td>\n      <td>4</td>\n      <td>97.0</td>\n      <td>67.0</td>\n      <td>1985</td>\n      <td>16.4</td>\n      <td>77</td>\n      <td>japan</td>\n      <td>subaru dl</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>22.0</td>\n      <td>4</td>\n      <td>121.0</td>\n      <td>76.0</td>\n      <td>2511</td>\n      <td>18.0</td>\n      <td>72</td>\n      <td>europe</td>\n      <td>volkswagen 411 (sw)</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>18.0</td>\n      <td>6</td>\n      <td>171.0</td>\n      <td>97.0</td>\n      <td>2984</td>\n      <td>14.5</td>\n      <td>75</td>\n      <td>usa</td>\n      <td>ford pinto</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>13.0</td>\n      <td>8</td>\n      <td>360.0</td>\n      <td>170.0</td>\n      <td>4654</td>\n      <td>13.0</td>\n      <td>73</td>\n      <td>usa</td>\n      <td>plymouth custom suburb</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>13.0</td>\n      <td>8</td>\n      <td>350.0</td>\n      <td>145.0</td>\n      <td>3988</td>\n      <td>13.0</td>\n      <td>73</td>\n      <td>usa</td>\n      <td>chevrolet malibu</td>\n    </tr>\n    <tr>\n      <th>215</th>\n      <td>13.0</td>\n      <td>8</td>\n      <td>318.0</td>\n      <td>150.0</td>\n      <td>3755</td>\n      <td>14.0</td>\n      <td>76</td>\n      <td>usa</td>\n      <td>dodge d100</td>\n    </tr>\n    <tr>\n      <th>309</th>\n      <td>41.5</td>\n      <td>4</td>\n      <td>98.0</td>\n      <td>76.0</td>\n      <td>2144</td>\n      <td>14.7</td>\n      <td>80</td>\n      <td>europe</td>\n      <td>vw rabbit</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>29.0</td>\n      <td>4</td>\n      <td>90.0</td>\n      <td>70.0</td>\n      <td>1937</td>\n      <td>14.0</td>\n      <td>75</td>\n      <td>europe</td>\n      <td>volkswagen rabbit</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>12.0</td>\n      <td>8</td>\n      <td>350.0</td>\n      <td>180.0</td>\n      <td>4499</td>\n      <td>12.5</td>\n      <td>73</td>\n      <td>usa</td>\n      <td>oldsmobile vista cruiser</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>26.0</td>\n      <td>4</td>\n      <td>116.0</td>\n      <td>75.0</td>\n      <td>2246</td>\n      <td>14.0</td>\n      <td>74</td>\n      <td>europe</td>\n      <td>fiat 124 tc</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>18.0</td>\n      <td>4</td>\n      <td>121.0</td>\n      <td>112.0</td>\n      <td>2933</td>\n      <td>14.5</td>\n      <td>72</td>\n      <td>europe</td>\n      <td>volvo 145e (sw)</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>13.0</td>\n      <td>8</td>\n      <td>400.0</td>\n      <td>175.0</td>\n      <td>5140</td>\n      <td>12.0</td>\n      <td>71</td>\n      <td>usa</td>\n      <td>pontiac safari (sw)</td>\n    </tr>\n    <tr>\n      <th>224</th>\n      <td>15.0</td>\n      <td>8</td>\n      <td>302.0</td>\n      <td>130.0</td>\n      <td>4295</td>\n      <td>14.9</td>\n      <td>77</td>\n      <td>usa</td>\n      <td>mercury cougar brougham</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>14.0</td>\n      <td>8</td>\n      <td>351.0</td>\n      <td>153.0</td>\n      <td>4129</td>\n      <td>13.0</td>\n      <td>72</td>\n      <td>usa</td>\n      <td>ford galaxie 500</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>21.0</td>\n      <td>4</td>\n      <td>120.0</td>\n      <td>87.0</td>\n      <td>2979</td>\n      <td>19.5</td>\n      <td>72</td>\n      <td>europe</td>\n      <td>peugeot 504 (sw)</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>14.0</td>\n      <td>8</td>\n      <td>302.0</td>\n      <td>137.0</td>\n      <td>4042</td>\n      <td>14.5</td>\n      <td>73</td>\n      <td>usa</td>\n      <td>ford gran torino</td>\n    </tr>\n    <tr>\n      <th>356</th>\n      <td>32.4</td>\n      <td>4</td>\n      <td>108.0</td>\n      <td>75.0</td>\n      <td>2350</td>\n      <td>16.8</td>\n      <td>81</td>\n      <td>japan</td>\n      <td>toyota corolla</td>\n    </tr>\n    <tr>\n      <th>288</th>\n      <td>18.2</td>\n      <td>8</td>\n      <td>318.0</td>\n      <td>135.0</td>\n      <td>3830</td>\n      <td>15.2</td>\n      <td>79</td>\n      <td>usa</td>\n      <td>dodge st. regis</td>\n    </tr>\n    <tr>\n      <th>201</th>\n      <td>18.5</td>\n      <td>6</td>\n      <td>250.0</td>\n      <td>110.0</td>\n      <td>3645</td>\n      <td>16.2</td>\n      <td>76</td>\n      <td>usa</td>\n      <td>pontiac ventura sj</td>\n    </tr>\n    <tr>\n      <th>392</th>\n      <td>27.0</td>\n      <td>4</td>\n      <td>151.0</td>\n      <td>90.0</td>\n      <td>2950</td>\n      <td>17.3</td>\n      <td>82</td>\n      <td>usa</td>\n      <td>chevrolet camaro</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>13.0</td>\n      <td>8</td>\n      <td>318.0</td>\n      <td>150.0</td>\n      <td>3940</td>\n      <td>13.2</td>\n      <td>76</td>\n      <td>usa</td>\n      <td>plymouth volare premier v8</td>\n    </tr>\n    <tr>\n      <th>308</th>\n      <td>33.5</td>\n      <td>4</td>\n      <td>151.0</td>\n      <td>90.0</td>\n      <td>2556</td>\n      <td>13.2</td>\n      <td>79</td>\n      <td>usa</td>\n      <td>pontiac phoenix</td>\n    </tr>\n    <tr>\n      <th>178</th>\n      <td>23.0</td>\n      <td>4</td>\n      <td>120.0</td>\n      <td>88.0</td>\n      <td>2957</td>\n      <td>17.0</td>\n      <td>75</td>\n      <td>europe</td>\n      <td>peugeot 504</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>14.0</td>\n      <td>8</td>\n      <td>340.0</td>\n      <td>160.0</td>\n      <td>3609</td>\n      <td>8.0</td>\n      <td>70</td>\n      <td>usa</td>\n      <td>plymouth 'cuda 340</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>18.0</td>\n      <td>6</td>\n      <td>232.0</td>\n      <td>100.0</td>\n      <td>3288</td>\n      <td>15.5</td>\n      <td>71</td>\n      <td>usa</td>\n      <td>amc matador</td>\n    </tr>\n    <tr>\n      <th>232</th>\n      <td>16.0</td>\n      <td>8</td>\n      <td>351.0</td>\n      <td>149.0</td>\n      <td>4335</td>\n      <td>14.5</td>\n      <td>77</td>\n      <td>usa</td>\n      <td>ford thunderbird</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>28.0</td>\n      <td>4</td>\n      <td>97.0</td>\n      <td>92.0</td>\n      <td>2288</td>\n      <td>17.0</td>\n      <td>72</td>\n      <td>japan</td>\n      <td>datsun 510 (sw)</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>10.0</td>\n      <td>8</td>\n      <td>307.0</td>\n      <td>200.0</td>\n      <td>4376</td>\n      <td>15.0</td>\n      <td>70</td>\n      <td>usa</td>\n      <td>chevy c20</td>\n    </tr>\n    <tr>\n      <th>383</th>\n      <td>38.0</td>\n      <td>4</td>\n      <td>91.0</td>\n      <td>67.0</td>\n      <td>1965</td>\n      <td>15.0</td>\n      <td>82</td>\n      <td>japan</td>\n      <td>honda civic</td>\n    </tr>\n    <tr>\n      <th>321</th>\n      <td>32.2</td>\n      <td>4</td>\n      <td>108.0</td>\n      <td>75.0</td>\n      <td>2265</td>\n      <td>15.2</td>\n      <td>80</td>\n      <td>japan</td>\n      <td>toyota corolla</td>\n    </tr>\n    <tr>\n      <th>169</th>\n      <td>20.0</td>\n      <td>6</td>\n      <td>232.0</td>\n      <td>100.0</td>\n      <td>2914</td>\n      <td>16.0</td>\n      <td>75</td>\n      <td>usa</td>\n      <td>amc gremlin</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>19.0</td>\n      <td>4</td>\n      <td>122.0</td>\n      <td>85.0</td>\n      <td>2310</td>\n      <td>18.5</td>\n      <td>73</td>\n      <td>usa</td>\n      <td>ford pinto</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>14.0</td>\n      <td>8</td>\n      <td>318.0</td>\n      <td>150.0</td>\n      <td>4457</td>\n      <td>13.5</td>\n      <td>74</td>\n      <td>usa</td>\n      <td>dodge coronet custom (sw)</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>35.0</td>\n      <td>4</td>\n      <td>72.0</td>\n      <td>69.0</td>\n      <td>1613</td>\n      <td>18.0</td>\n      <td>71</td>\n      <td>japan</td>\n      <td>datsun 1200</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>19.0</td>\n      <td>6</td>\n      <td>250.0</td>\n      <td>100.0</td>\n      <td>3282</td>\n      <td>15.0</td>\n      <td>71</td>\n      <td>usa</td>\n      <td>pontiac firebird</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Perform t`est train split\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Build a Ridge, Lasso and regular linear regression model. \n",
    "# Note how in scikit learn, the regularization parameter is denoted by alpha (and not lambda)\n",
    "ridge = Ridge(alpha=0.5)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unpenalized Linear Regression Coefficients are:[[ -2.89775588   7.76859732 -11.37387644 -13.66319985  -0.77064653\n    8.41471347]]\nUnpenalized Linear Regression Intercept:[26.55043675]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unpenalized Linear Regression Coefficients are:{}\".format(lin.coef_))\n",
    "print(\"Unpenalized Linear Regression Intercept:{}\".format(lin.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lasso Regression Coefficients are:[-1.05828090e+01 -0.00000000e+00 -0.00000000e+00 -2.66519248e-03\n  0.00000000e+00  3.35795166e-01]\nLasso Linear Regression Intercept:[27.00065828]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso Regression Coefficients are:{}\".format(lasso.coef_))\n",
    "print(\"Lasso Linear Regression Intercept:{}\".format(lasso.intercept_))"
   ]
  },
  {
   "source": [
    "effect of betas are reduced for Ridge"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ridge Regression Coefficients are:[[-3.42728883 -1.72492322 -4.43716534 -8.13217993 -0.35374782  6.44639513]]\nRidge Linear Regression Intercept:[26.24575421]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ridge Regression Coefficients are:{}\".format(ridge.coef_))\n",
    "print(\"Ridge Linear Regression Intercept:{}\".format(ridge.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 39 into shape (40,1)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-31a3d4bb009b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_h_ridge_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my_h_lasso_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_h_lasso_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    299\u001b[0m            [5, 6]])\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 39 into shape (40,1)"
     ]
    }
   ],
   "source": [
    "# create predictions\n",
    "y_h_ridge_train = ridge.predict(X_train)\n",
    "y_h_ridge_test = ridge.predict(X_test)\n",
    "\n",
    "y_h_lasso_train = np.reshape(lasso.predict(X_train),(40,1))\n",
    "y_h_lasso_test = np.reshape(lasso.predict(X_test),(10,1))\n",
    "\n",
    "y_h_lin_train = lin.predict(X_train)\n",
    "y_h_lin_test = lin.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_h_ridge_train.shape)\n",
    "print(y_h_ridge_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(type(y_h_lasso_train))\n",
    "print(type(y_h_ridge_train))"
   ]
  },
  {
   "source": [
    "errror is lower because did not divide by n\n",
    "higher training error for Ridge and Lasso, but greater predictive value\n",
    "what if we increased dimension?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the Residual for Ridge, Lasso, and Unpenalized Regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine the residual sum of sq\n",
    "print('Train Error Ridge Model', np.sum((y_train - y_h_ridge_train)**2))\n",
    "print('Test Error Ridge Model', np.sum((y_test - y_h_ridge_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Lasso Model', np.sum((y_train - y_h_lasso_train)**2))\n",
    "print('Test Error Lasso Model', np.sum((y_test - y_h_lasso_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Unpenalized Linear Model', np.sum((y_train - lin.predict(X_train))**2))\n",
    "print('Test Error Unpenalized Linear Model', np.sum((y_test - lin.predict(X_test))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Ridge and Lasso Perform in Higher Dimensional Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 degree polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try polynomial features on the regression \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#instantiate this class\n",
    "poly_2 = PolynomialFeatures(degree=2, interaction_only=False)\n",
    "#fit and transform the data and create a  new dataframe\n",
    "df_poly= pd.DataFrame(poly_2.fit_transform(X), columns=poly_2.get_feature_names(X.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(50, 28)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "df_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "X_train , X_test, y_train, y_test = train_test_split(df_poly, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Build a Ridge, Lasso and regular linear regression model. \n",
    "# Note how in scikit learn, the regularization parameter is denoted by alpha (and not lambda)\n",
    "ridge = Ridge(alpha=0.3)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=0.3)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unpenalized Linear Regression Coefficients are:[[ 2.00993189e+14 -9.98890117e+01  9.38927485e+01  2.39649242e+02\n  -1.32397879e+02  5.12563715e+01  2.13449395e+01  1.36178578e+01\n   1.45446325e+02  1.28575061e+02 -2.43185771e+02  1.72136123e+02\n   4.44129485e+01 -1.47695770e+02 -4.80107850e+02  3.66349816e+02\n  -1.48087464e+02 -2.34127215e+01 -1.79128780e+02  4.21083897e+02\n  -3.56102311e+02 -9.82518366e+01 -1.01948357e+02  1.36592666e+02\n   9.84205527e+00 -2.02670710e+01 -5.54587409e+01  2.77741951e+01]]\nUnpenalized Linear Regression Intercept:[-2.00993189e+14]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unpenalized Linear Regression Coefficients are:{}\".format(lin.coef_))\n",
    "print(\"Unpenalized Linear Regression Intercept:{}\".format(lin.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lasso Regression Coefficients are:[ 0.         -7.62362312 -0.         -0.         -9.51133123 -0.\n  0.         -0.         -0.         -0.         -0.         -0.\n -0.         -0.         -0.         -0.         -0.         -0.\n -0.         -0.         -0.         -0.         -0.         -0.\n -0.         -0.          0.          8.75114956]\nLasso Linear Regression Intercept:[27.45117281]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso Regression Coefficients are:{}\".format(lasso.coef_))\n",
    "print(\"Lasso Linear Regression Intercept:{}\".format(lasso.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ridge Regression Coefficients are:[[ 0.         -3.86601887 -5.65038648 -3.7945169  -7.98028181 -0.96381617\n   4.61859091 -1.75689815  0.85293701 -0.63892742  2.2483479   1.50704674\n  -0.93060138  0.96296962 -0.0511033   1.36784921 -2.47140586 -2.36024786\n  -0.76080271  0.22958767 -1.12039238 -2.24204625  1.17177304 -4.4594435\n  -4.40293735 -2.83756135 -1.30980967  9.14223447]]\nRidge Linear Regression Intercept:[29.42863416]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ridge Regression Coefficients are:{}\".format(ridge.coef_))\n",
    "print(\"Ridge Linear Regression Intercept:{}\".format(ridge.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions\n",
    "y_h_ridge_train = ridge.predict(X_train)\n",
    "y_h_ridge_test = ridge.predict(X_test)\n",
    "\n",
    "y_h_lasso_train = np.reshape(lasso.predict(X_train),(40,1))\n",
    "y_h_lasso_test = np.reshape(lasso.predict(X_test),(10,1))\n",
    "\n",
    "y_h_lin_train = lin.predict(X_train)\n",
    "y_h_lin_test = lin.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Error Ridge Model mpg    324.842909\ndtype: float64\nTest Error Ridge Model mpg    53.73432\ndtype: float64\n\n\nTrain Error Lasso Model mpg    526.199294\ndtype: float64\nTest Error Lasso Model mpg    62.242381\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    233.090586\ndtype: float64\nTest Error Unpenalized Linear Model mpg    3237.695742\ndtype: float64\n"
     ]
    }
   ],
   "source": [
    "# examine the residual sum of sq\n",
    "print('Train Error Ridge Model', np.sum((y_train - y_h_ridge_train)**2))\n",
    "print('Test Error Ridge Model', np.sum((y_test - y_h_ridge_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Lasso Model', np.sum((y_train - y_h_lasso_train)**2))\n",
    "print('Test Error Lasso Model', np.sum((y_test - y_h_lasso_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Unpenalized Linear Model', np.sum((y_train - lin.predict(X_train))**2))\n",
    "print('Test Error Unpenalized Linear Model', np.sum((y_test - lin.predict(X_test))**2))"
   ]
  },
  {
   "source": [
    "in penalizing variables when there are fewer factors, in some situations, you need the variability of every single predictor, where in higher degree polynomials you want to get rid of some of those extra variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "data is overfit, but Lasso and Ridge reduced testing error"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Even higher degree polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(50, 462)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "poly_5 = PolynomialFeatures(degree=5, interaction_only=False)\n",
    "#fit and transform the data and create a  new dataframe\n",
    "df_poly_5= pd.DataFrame(poly_5.fit_transform(X), columns=poly_5.get_feature_names(X.columns))\n",
    "df_poly_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "X_train , X_test, y_train, y_test = train_test_split(df_poly_5, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Build a Ridge, Lasso and regular linear regression model. \n",
    "# Note how in scikit learn, the regularization parameter is denoted by alpha (and not lambda)\n",
    "ridge = Ridge(alpha=1)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions\n",
    "y_h_ridge_train = ridge.predict(X_train)\n",
    "y_h_ridge_test = ridge.predict(X_test)\n",
    "\n",
    "y_h_lasso_train = np.reshape(lasso.predict(X_train),(40,1))\n",
    "y_h_lasso_test = np.reshape(lasso.predict(X_test),(10,1))\n",
    "\n",
    "y_h_lin_train = lin.predict(X_train)\n",
    "y_h_lin_test = lin.predict(X_test)"
   ]
  },
  {
   "source": [
    "lasso isosurface is square, while ridge isosurface is circle\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Error Ridge Model mpg    349.869954\ndtype: float64\nTest Error Ridge Model mpg    76.76013\ndtype: float64\n\n\nTrain Error Lasso Model mpg    1159.688981\ndtype: float64\nTest Error Lasso Model mpg    168.514648\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2.334650e-25\ndtype: float64\nTest Error Unpenalized Linear Model mpg    935.214829\ndtype: float64\n"
     ]
    }
   ],
   "source": [
    "# examine the residual sum of sq\n",
    "print('Train Error Ridge Model', np.sum((y_train - y_h_ridge_train)**2))\n",
    "print('Test Error Ridge Model', np.sum((y_test - y_h_ridge_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Lasso Model', np.sum((y_train - y_h_lasso_train)**2))\n",
    "print('Test Error Lasso Model', np.sum((y_test - y_h_lasso_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Unpenalized Linear Model', np.sum((y_train - lin.predict(X_train))**2))\n",
    "print('Test Error Unpenalized Linear Model', np.sum((y_test - lin.predict(X_test))**2))"
   ]
  },
  {
   "source": [
    "indicative of overfitting, because it penalizes parameters that are not helpful in this case"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating AIC and BIC \n",
    "AIC and BIC are information criteria for evaluating how good of a model is by giving a measurement of parsimony and goodness of fit. \n",
    "\n",
    "- AIC is defined as: $2k - 2log(L)$\n",
    "- BIC is defined as: $klog(n) - 2log(L)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aic(y, y_pred, k):\n",
    "    resid = y - y_pred\n",
    "    sse = (resid**2).sum()\n",
    "    AIC = 2*k - 2*np.log(sse)\n",
    "    \n",
    "    return AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poly_5.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aic(y_test, y_h_lasso_test, df_poly_5.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aic(y_test, y_h_ridge_test, df_poly_5.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aic(y_test, y_h_lin_test, df_poly_5.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}