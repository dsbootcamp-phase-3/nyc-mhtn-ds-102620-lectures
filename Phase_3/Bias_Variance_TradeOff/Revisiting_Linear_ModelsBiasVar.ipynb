{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions to Linear Models\n",
    "\n",
    "_December 8, 2020_\n",
    "\n",
    "Agenda today:\n",
    "- Overfitting and Underfitting: Bias Variance Tradeoff\n",
    "- Validating your model: K-fold Cross Validation \n",
    "    - Validation set\n",
    "    - Leave-one-out cross validation (LOOCV)\n",
    "    - K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "underfit has high bias<br />\n",
    "overfit has high variance<br />\n",
    "\n",
    "bias is difference between average prediction and actual value<br />\n",
    "bias pays little attention to training data<br />\n",
    "variance pays attention to training data and does not generalize on data but has high error rates on test data<br />\n",
    "model becomes less complex and has greater bias and underfit<br />\n",
    "model becomes more complex and has greater variance and overfit<br />\n",
    "\n",
    "sometimes you can take Bo as an indiccation of high bias, which is the intercept<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Bias and Variance \n",
    "\n",
    "There are 3 types of prediction error: bias, variance, and irreducible error.\n",
    "\n",
    "**Total Error = Bias^2 + Variance + Irreducible Error**\n",
    "\n",
    "In module 2 project, you might have built a regression model that performed really well on your data, that is highly complex. However, does it really have good generalizability to unseen data? That is the problem of tradeoff: we want a model that does not fit to every squiggle of the data but learns the general and underlying pattern such that it can perform well on unseen data. So what is bias and what is variance?\n",
    "\n",
    "#### Bias\n",
    "Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the underlying pattern. It always leads to high error on training and test data.\n",
    "\n",
    "Models with high bias tend to underfit, which doesn't learn the signal of our training data and of course, generalize poorly to testing data.\n",
    "\n",
    "#### Variance\n",
    "Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to variability training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.\n",
    "\n",
    "Models with high variance tend to overfit, which learns all the noises of the training data and generalizes poorly to the testing data. \n",
    "\n",
    "\n",
    "<img src=\"attachment:Screen%20Shot%202019-03-07%20at%202.23.37%20PM.png\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Bias-vs.-Variance-v5-2-darts.png' width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High bias** algorithms tend to be less complex, with simple or rigid underlying structure.\n",
    "\n",
    "+ They train models that are consistent, but inaccurate on average.\n",
    "+ These include linear or parametric algorithms such as regression and naive Bayes.\n",
    "\n",
    "On the other hand, **high variance** algorithms tend to be more complex, with flexible underlying structure.\n",
    "\n",
    "+ They train models that are accurate on average, but inconsistent.\n",
    "+ These include non-linear or non-parametric algorithms such as decision trees and nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Bias-vs.-Variance-v4-chart.png' width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "underfit - high on training set and high on training set<br />\n",
    "overfit - low on training set and high on training set<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which ones of the plots represent high bias vs high variance?\n",
    "\n",
    "Correspondingly, which model is overfitted and which one is underfitted?\n",
    "\n",
    "<img src='noisy-sine-linear.png' width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='noisy-sine-decision-tree.png' width=500/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachment:Screen%20Shot%202019-03-07%20at%202.23.37%20PM.png\" style=\"width:500px;\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachment:Screen%20Shot%202019-03-07%20at%2011.30.43%20AM.png\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A well fitted model\n",
    "<img src='noisy-sine-third-order-polynomial.png' width=500 />\n",
    "We want to try to find the proper balance of variance and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics - MSE and RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with Test and Train Split\n",
    "\n",
    "You could have a bad random sample that isn't representative of the population<br />\n",
    "Information loss<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE and RMSE\n",
    "MSE stands for mean squared error, it is the variance of the estimated values from the actual value. RMSE stands for root-mean square error, and it is the standard deviation of the residual. The RMSE is a measure of how much each prediction is away from the actual y value. The smaller RMSE is, the closer the prediction is to the actual values. Note that RMSE and MSE is unstandardized--meaning it is calculated in the original unit of the values measure. Therefore, you can have a good $R^2$ but still a high RMSE if the original unit of measurement is large.\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum(Y_i - \\hat Y_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Cross Validation\n",
    "What are some of the obvious problems with using train test split?\n",
    "In order to tackle this problem, we need cross validation. There are a few ways of working with cross validation\n",
    "\n",
    "#### 1. Train Test Split\n",
    "As you probably have guessed it, train test split is splitting the data into a training set, for fitting the model, and a validation, or hold-out, or testing set. The fitted model is then used for prediction on the validation set. The resulting validation set error rate--typically assesed using MSE, provides an estimate of the test error rate. The advantage of train test split is apparent - we no longer have an overfitted model trained and tested on the same dataset. However, what could be some of the drawback of using train test split?\n",
    "\n",
    "\n",
    "#### 2. Leave one out cross validation\n",
    "The Leave-One-Out Cross Validation is an approach it address the drawback of train-test-split. Instead of partitioning the data into subsets of equal sizes, we hold out one single observation $(x_1,y_1)$ as the validation set, and the remaining observations are used for training the model. The MSE will be evaluated as $(\\hat y - \\bar y)^2$. We will then use this approach on every single observation and compute MSE for the $nth$ term and average them, giving the Leave one out MSE as:\n",
    "\n",
    "\n",
    "<center>   $CV = \\frac{1}{n} \\sum MSE_i$\n",
    "\n",
    "What are some of the drawback regarding this method of cross-validation?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=attachment:Screen%20Shot%202019-03-07%20at%203.36.27%20PM.png style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is used as training and testing, and would take a long time\n",
    "Not very useful for large datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. K-fold Cross Validation\n",
    "What is the disadvantage of LOOCV? As you can imagine, this method can be costly to implement, especially when the sample size $n$ is very large. Let's take a look at another, perhaps most commonly used validation method--k-fold validation. \n",
    "\n",
    "This approach is an alternative to LOOCV and it involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit the model on the remaining k − 1 folds. The mean squared error, $MSE_1$, is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set. This process results in k estimates of the test error, MSE1, MSE2, . . . , MSEk. The k-fold CV estimate is computed by averaging these testing error.\n",
    "\n",
    "<img src=\"attachment:Screen%20Shot%202019-03-11%20at%207.34.04%20AM.png\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterate through the datasets, and then average the results<br />\n",
    "To make sure you dont have an unrepresentative training set<br />\n",
    "if you use the same algorithm, you are using the same model but if you are choosing different subset you are creating a different instance<br />\n",
    "algorithm is recipe<br />\n",
    "model is the dish<br />\n",
    "allows you to choose between models and not between different instances<br />\n",
    "different models like linear regression and logarithmic regression<br />\n",
    "simple models like linear and decision tree, you can use K-fold for cross-validation<br />\n",
    "beta refers to how many variables you have<br />\n",
    "weights do change in terms and are in the process of gradient descent, while  coefficients do not change\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
